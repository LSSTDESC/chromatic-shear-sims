#!/usr/bin/env python

import argparse
import functools
import logging
from logging import handlers
import multiprocessing
import threading
import copy
import os

import galsim
import numpy as np
import pyarrow as pa
import pyarrow.compute as pc
import pyarrow.dataset as ds
import pyarrow.parquet as pq
import pyarrow.feather as ft
import yaml

from chromatic_shear_sims import run_utils
from chromatic_shear_sims import logging_config
from chromatic_shear_sims.pipeline import Pipeline


logger = logging.getLogger(__name__)


# https://docs.python.org/3/howto/logging-cookbook.html#logging-to-a-single-file-from-multiple-processes
def logger_thread(queue):
    while True:
        record = queue.get()
        if record is None:
            break
        logger = logging.getLogger(record.name)
        logger.handle(record)


def initializer(queue, log_level):
    queue_handler = handlers.QueueHandler(queue)
    logger = logging.getLogger()
    logger.setLevel(log_level)
    logger.addHandler(queue_handler)
    logger.debug(f"spawning worker process")


def run_pipeline(config, seed=None):
    rng = np.random.default_rng(seed)

    pipeline = Pipeline(config)

    pipeline.load()
    pipeline.load_survey()
    pipeline.load_galaxies()
    pipeline.load_stars()

    measure_config = pipeline.config.get("measure")
    measure_type = measure_config.get("type")

    # psf
    psf = pipeline.get_psf()

    color, colors = pipeline.get_colors()

    # stars
    if colors:
        chroma_stars = [
            pipeline.stars(color)
            for color in colors
        ]
        star = chroma_stars[1]
    elif color:
        star = pipeline.stars(color)
    else:
        star = pipeline.stars(color)

    # scene & image
    image_config = pipeline.config.get("image")
    scene_pos = pipeline.get_scene_pos(seed=seed)
    n_gals = len(scene_pos)

    # galaxies
    galaxies = pipeline.galaxies(n_gals)

    scene = [
        gal.shift(pos)
        for (gal, pos) in zip(galaxies, scene_pos)
    ]

    pair_seed = rng.integers(1, 2**64 // 2 - 1)
    bands = image_config["bands"]
    shear = image_config["shear"]

    pair = run_utils.build_pair(
        pipeline.survey,
        scene,
        star,
        shear,
        psf,
        bands,
        image_config["xsize"],
        image_config["ysize"],
        image_config["psf_size"],
        pair_seed,
    )

    # measure
    meas_seed = rng.integers(1, 2**64 // 2 - 1)

    match measure_type:
        case "metadetect":
            shear_bands = measure_config.get("shear_bands")
            det_bands = measure_config.get("det_bands")
            batches = run_utils.run_pair(
                pipeline,
                pair,
                psf,
                bands,
                shear_bands,
                det_bands,
                pipeline.metadetect_config,
                meas_seed,
            )
        case "chromatic_metadetect":
            raise ValueError(f"Not working at the moment!")
            shear_bands = measure_config.get("shear_bands")
            det_bands = measure_config.get("det_bands")
            # FIXME return batches...
            measurement = run_utils.measure_pair_color(
                pair,
                psf,
                colors,
                chroma_stars,
                image_config["psf_size"],
                pipeline.survey.scale,
                bands,
                shear_bands,
                det_bands,
                pipeline.metadetect_config,
                meas_seed,
            )
        case "drdc":
            shear_bands = measure_config.get("shear_bands")
            det_bands = measure_config.get("det_bands")
            batches = run_utils.run_pair_color_response(
                pipeline,
                pair,
                psf,
                colors,
                chroma_stars,
                image_config["psf_size"],
                bands,
                shear_bands,
                det_bands,
                pipeline.metadetect_config,
                meas_seed,
            )
        case _:
            raise ValueError(f"Measure type {measure_type} not valid!")

    return batches


def get_args():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--config",
        type=str,
        required=True,
        help="configuration file [yaml]",
    )
    parser.add_argument(
        "--seed",
        type=int,
        required=False,
        default=None,
        help="RNG seed [int; None]",
    )
    parser.add_argument(
        "--output",
        type=str,
        required=False,
        default=".",
        help="Output directory [str; .]"
    )
    parser.add_argument(
        "--n_sims",
        type=int,
        required=False,
        default=1,
        help="Number of sims to run [int; 1]"
    )
    parser.add_argument(
        "--n_jobs",
        type=int,
        required=False,
        default=1,
        help="Number of parallel jobs to run [int; 1]"
    )
    parser.add_argument(
        "--log_level",
        type=int,
        required=False,
        default=2,
        help="logging level [int; 2]",
    )
    return parser.parse_args()


def main():
    args = get_args()

    log_level = logging_config.get_level(args.log_level)
    logging_config.setup_logging(logger, logging.root, __name__, log_level)

    logger.info(f"{vars(args)}")

    config = args.config
    seed = args.seed
    n_sims = args.n_sims
    n_jobs = args.n_jobs
    output = args.output

    pipeline = Pipeline(config)

    pipeline.load()
    pipeline.load_survey()
    pipeline.load_galaxies()
    pipeline.load_stars()

    for aggregate_k, aggregate_v in pipeline.aggregates.items():
        logger.info(f"{aggregate_k}: {aggregate_v}")

    output_path = os.path.join(
        output,
        pipeline.name,
    )
    os.makedirs(output_path, exist_ok=True)
    output_format = "parquet"

    schema = pipeline.get_schema()

    rng = np.random.default_rng(seed)

    logger.info(f"running simulations and writing data to {output_path}")
    # jobs = []
    # for _seed in rng.integers(1, 2**32, n_sims):
    #     jobs.append(
    #         joblib.delayed(run_pipeline)(
    #             config, seed=_seed
    #         )
    #     )
    # # with joblib.Parallel(n_jobs=n_jobs, verbose=100, return_as="generator") as parallel:
    # #     res_generator = parallel(jobs)
    # batches = joblib.Parallel(n_jobs=n_jobs, verbose=10, return_as="generator")(jobs)
    # # with joblib.Parallel(n_jobs=n_jobs, verbose=100, return_as="list") as parallel:
    # #     _batches = parallel(jobs)
    # # batches = iter(_batches)
    # chained = chain.from_iterable(batches)

    # queue = multiprocessing.Manager().Queue(-1)
    multiprocessing.set_start_method("spawn")
    # ctx = multiprocessing.get_context("spawn")  # spawn is ~ fork-exec
    # queue = ctx.Queue(-1)
    queue = multiprocessing.Queue(-1)

    lp = threading.Thread(target=logger_thread, args=(queue,))
    lp.start()

    seeds = rng.integers(1, 2**32, n_sims)
    with pq.ParquetWriter(f"{output_path}/{seed}.parquet", schema) as writer:
        # with concurrent.futures.ProcessPoolExecutor(
        #     max_workers=n_jobs,
        #     mp_context=ctx,
        #     initializer=initializer,
        #     initargs=(queue, log_level),
        #     # max_tasks_per_child=1,  # more tasks lead to a thread lock I don't understand...
        #     max_tasks_per_child=n_sims // n_jobs,
        # ) as executor:
        #     for batches in executor.map(functools.partial(run_pipeline, config), seeds):
        #         logger.info(f"writing batches")
        #         for batch in batches:
        #             writer.write_batch(batch)
        with multiprocessing.Pool(
            n_jobs,
            initializer=initializer,
            initargs=(queue, log_level),
            maxtasksperchild=n_sims // n_jobs,
            # context=ctx,
        ) as pool:
            for i, batches in enumerate(pool.imap(functools.partial(run_pipeline, config), seeds)):
                ii = i + 1
                logger.info(f"finished simulation {ii}")
                n_batches_written = 0
                for batch in batches:
                    writer.write_batch(batch)
                    n_batches_written += 1
                    logger.debug(f"{n_batches_written} batches written for simulation {ii}")

    # chained = chain.from_iterable(batches_generator)
    # # logger.info(f"Constructing record batch reader")
    # # batch_reader = pa.RecordBatchReader.from_batches(
    # #     schema,
    # #     chain.from_iterable(batches),
    # # )

    # ds.write_dataset(
    #     # batches_generator,
    #     chained,
    #     # batches,
    #     # batch_reader,
    #     output_path,
    #     basename_template=f"{seed}-part-{{i}}.{output_format}",
    #     format=output_format,
    #     schema=schema,
    #     existing_data_behavior="overwrite_or_ignore",
    #     max_rows_per_file=1024**2 * 100,
    # )

    queue.put(None)
    lp.join()

    logger.info(f"simulations completed")


if __name__ == "__main__":
    main()
