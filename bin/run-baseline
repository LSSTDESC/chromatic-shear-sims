#!/usr/bin/env python
"""
"""

import argparse
from datetime import datetime as dt
import functools
import itertools
import os
from pathlib import Path
import re
import time

import galsim
import joblib
import metadetect
import ngmix
import numpy as np
import pyarrow as pa
import pyarrow.compute as pc
import pyarrow.dataset as ds
import pyarrow.parquet as pq
from tqdm import tqdm, trange
import yaml

from chromatic_shear_bias import sed_tools, run_utils, lsst

from chromatic_shear_bias.generators import generators, gals, stars


def main():
    args = get_args()

    with open(args.config, "r") as fp:
        config = yaml.safe_load(fp.read())

    simple_gals = gals.simple_gal_generator()

    psf = galsim.Gaussian(fwhm=0.9)

    rng = np.random.default_rng(args.seed)

    xsize = 640
    ysize = 640
    psf_size = 53
    pixel_scale = 0.2
    shear = 0.02

    bands = ["g", "r", "i"]
    noises = [
        run_utils.get_sky_rms(
            lsst.exposure_time,
            lsst.zeropoint[band],
            lsst.sky_brightness[band],
            pixel_scale,
        )
        for band in bands
    ]
    shear_bands = [[1]]
    det_bands = [[0, 1, 2]]

    # We break down all of the jobs into chunks proportional to the number of
    # jobs. This balances between configuring each scene in serial and
    # simulating and measuring each scene in parallel.

    chunk_factor = 4
    job_chunks = [chunk_factor * args.n_jobs for i in range(args.n_sims // (chunk_factor * args.n_jobs))]
    if args.n_sims % (chunk_factor * args.n_jobs) > 0:
        job_chunks.append(args.n_sims % (chunk_factor * args.n_jobs))
    # job_chunks = [args.n_sims]
    print(f"Running {args.n_sims} jobs in {len(job_chunks)} chunks")

    start_time = time.time()
    n_finished = 0
    results = []
    parallel = joblib.Parallel(n_jobs=args.n_jobs, verbose=10)
    for chunk_index, chunk in enumerate(job_chunks):
        print(f"Making {chunk} jobs for chunk {chunk_index + 1} / {len(job_chunks)}")
        jobs = []
        for i in trange(chunk, ncols=80):
            scene_seed = rng.integers(1, 2**64 // 2 - 1)
            pair_seed = rng.integers(1, 2**64 // 2 - 1)
            meas_seed = rng.integers(1, 2**64 // 2 - 1)
            scene = generators.build_scene(
                simple_gals,
                xsize,
                ysize,
                pixel_scale,
                scene_seed,
                mag=args.mag
            )
            _star = galsim.DeltaFunction()
            jobs.append(
                joblib.delayed(generators.build_and_measure_pair)(
                    scene,
                    _star,
                    shear,
                    xsize,
                    ysize,
                    psf_size,
                    pixel_scale,
                    bands,
                    noises,
                    psf,
                    args.n_coadd,
                    shear_bands,
                    det_bands,
                    config,
                    pair_seed,
                    meas_seed
                )
            )

        res = parallel(jobs)
        results.extend(res)

        n_finished += chunk
        current_time = time.time()
        elapsed_time = current_time - start_time
        elapsed_time_formatted = dt.strftime(dt.utcfromtimestamp(elapsed_time), '%H:%M:%S')
        print(f"Completed {n_finished} jobs; elapsed time: {elapsed_time_formatted}")

    print(f"Writing output")
    # output file path
    root_path = Path(args.output)
    # if root_path.exists() and not root_path.is_dir():
    #     raise RuntimeError(f"{root_path} exists but is not a directory!")
    if not root_path.exists():
        root_path.mkdir(exist_ok=True)
    # path = root_path / f"{seed}.parquet"

    schema = run_utils._get_schema()
    batches = (pa.RecordBatch.from_pylist(res, schema) for res in results if res)
    # table = pa.Table.from_batches(batches, schema)
    pqwriter = pq.ParquetWriter(
        root_path / f"{args.seed}.parquet",
        schema,
    )
    # pqwriter.write_table(table)
    for batch in batches:
        pqwriter.write_batch(batch)

    # records = []
    # num_rows = 0
    # for res in results:
    #     if res:
    #         records.append(res)
    #         num_rows += len(res)
    #     if num_rows > 1_000_000:
    #         batch = pa.RecordBatch.from_pylist(records, schema)
    #         pqwriter.write_batch(batch)
    #         records = []
    #         num_rows = 0
    # if num_rows > 0:
    #     batch = pa.RecordBatch.from_pylist(records, schema)
    #     pqwriter.write_batch(batch)
    #     records = []
    #     num_rows = 0

    pqwriter.close()

    print(f"Finished")


def get_args():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--config",
        type=str,
        required=True,
        help="Metadetection configuration file [yaml]",
    )
    parser.add_argument(
        "--mag",
        type=float,
        required=False,
        default=None,
        help="r-band magnitude at which to draw galaxies",
    )
    parser.add_argument(
        "--n_coadd",
        type=int,
        required=False,
        default=100,
        help="Number of exposures in coadd [int]",
    )
    parser.add_argument(
        "--seed",
        type=int,
        required=False,
        default=1,
        help="RNG seed [int]",
    )
    parser.add_argument(
        "--n_sims",
        type=int,
        required=False,
        default=1,
        help="Number of sims to run [int]",
    )
    parser.add_argument(
        "--n_jobs",
        type=int,
        required=False,
        default=joblib.cpu_count(),
        # default=max(1, joblib.cpu_count() // 2),
        help="Number of jobs to run [int]",
    )
    parser.add_argument(
        "--output",
        type=str,
        required=False,
        default="",
        help="Output directory"
    )
    return parser.parse_args()


if __name__ == "__main__":
    main()
